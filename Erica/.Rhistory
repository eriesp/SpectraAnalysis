norm<-function (n, alpha)
{
vec <- vector("numeric", n+1)
x = 0
#        x <- -2*alpha  # RUN the algorithm by yourself with this value as the initial point!
accept=0
vec[1] <- x
for (i in 2:n) {
innov <- runif(1, -alpha, alpha)
y <- x + innov
aprob <- min(1, dnorm(y)/dnorm(x))
u <- runif(1)
# If condition "(u < aprob)" is NOT met, we'll skip command "x <- y",
#  so that the MC does not move from x
if (u < aprob)
{
x <- y
accept1=accept+1
accept=accept1
}
vec[i] <- x
}
vec[n+1]= accept/n
vec
}
x11()
par(mfrow=c(1,2))
set.seed(321)
plot(ts(norm(10,2)),type='p')
plot(ts(norm(10,2)))
#################################
# Number of iterations of the MC
k = 10000
##################### alpha=1
normvec<-norm(k,1)
normvec1 = normvec[1:k]
rate1 = normvec[(k+1)]
x11()
par(mfrow=c(1,2))
plot(ts(normvec1))
hist(normvec1,30, prob=T)
val=seq(-3,3,0.1)
points(val,dnorm(val),type='l',col='red')
help(ts)
windows()
plot(density(normvec1),bw='nrd')
points(val,dnorm(val),type='l',col='red')
burnin=1000 # usually we get rid of the first burnin iterations,
# where the chain hasn't reached stationarity yet
# The ergodic Th says we can use all iteration from any initial
# point x. However, we are averaging some function of some finite
# number of samples, so that out average will be a better
# approximation if we start at a typical point in the density we
# are sampling from
# and if this density is closer to the target distribution
b1=burnin+1
normveclast1=normvec[b1:k]
par(mfrow=c(1,1))
plot(density(normveclast1))
points(val,dnorm(val),type='l',col='red')
rate1
windows()
################## alpha=10
normvec<-norm(k,10)
normvec10 = normvec[1:k]
rate10 = normvec[(k+1)]
x11()
par(mfrow=c(1,2))
plot(ts(normvec10))
hist(normvec10,30,prob=T)
points(val,dnorm(val),type='l',col='red')
rate10
windows()
normvec<-norm(k,100)
normvec100 = normvec[1:k]
rate100 = normvec[(k+1)]
par(mfrow=c(1,2))
plot(ts(normvec100))
hist(normvec100,30,prob=T)
points(val,dnorm(val),type='l',col='red')
rate100
windows()
par(mfrow=c(1,3))
plot(density(normvec1),main="alpha=1")
points(val,dnorm(val),type='l',col='red')
plot(density(normvec10),main="alpha=10")
points(val,dnorm(val),type='l',col='red')
plot(density(normvec100),main="alpha=100")
points(val,dnorm(val),type='l',col='red')
windows()
par(mfrow=c(1,3))
plot(ts(normvec1[1000:1500]))
plot(ts(normvec10[1000:1500]))
plot(ts(normvec100[1000:1500]))
## COMPARISON among acceptance rates of the simulated MCs for different values of alpha
rate1;rate10;rate100
##################################################################
# EXAMPLE 2 - Ex in 6.7 in Albert
##################################################################
# Section 6.7 Learning about a Normal Population from Grouped Data
##################################################################
#Example of a MH algorithm
rm(list=ls())
library(LearnBayes)
# d is a list defining the intervals and the frequencies
d=list(int.lo=c(-Inf,seq(66,74,by=2)),int.hi=c(seq(66,74,by=2), Inf),f=c(14,30,49,70,33,15))
d
help(groupeddatapost)
# EX: compute the log-posterior density at  mu=70, log(sigma)=1
groupeddatapost(c(70,1),d)
help(laplace)
start=c(70,1) #punto iniziale - costruito da dati "fittizi"
fit=laplace(groupeddatapost,start,d)
fit
diag(fit$var)
modal.sds=sqrt(diag(fit$var))
### Otherwise use the function optim
?optim
####################### METROPOLIS-HASTINGS  ALGORITHM ################################
# Build the MC according to a RANDOM WALK Metropolis algorithm:
# the bivariate proposal density q is N(val_prec,c^2 V)
# where the scale factor c=2, V  is the covariance matrix (\tilde I_n)^{-1};
# this is the covariance of the Gaussian approximation
# The PROPOSAL distribution has to be close to the true posterior, i.e.
#   it should be an approximation of the posterior!!!
# The scale factor c determines the behaviour of the MC
######################proposal=list(var=fit$var,scale=2)
proposal=list(var=fit$var,scale=2) #2
proposal
fit2=rwmetrop(groupeddatapost,proposal,start,10000,d)
View(fit2)
View(d)
fit2
fit2$accept
# Ergodic means of the two parameters
#help(apply)
# Posterior means (calcolate col metodo MCMC) of mu and log(sigma)
post.means=apply(fit2$par,2,mean)
# Posterior Standard deviation  of the two marginal posteriors (MCMC)
post.sds=apply(fit2$par,2,sd)
post.means
post.sds
#Confronto fra medie e varianze dell'approx gaussiana con quelle a posteriori
cbind(c(fit$mode),modal.sds)
cbind(post.means,post.sds)
x11()
mycontour(groupeddatapost,c(69,71,.6,1.3),d)
points(fit2$par[5001:10000,1],fit2$par[5001:10000,2])
windows()
## marginal traceplots
par(mfrow=c(1,2))
plot(ts(fit2$par[5001:10000,1]),ylab="mu")
plot(ts(fit2$par[5001:10000,2]),ylab="sigma")
